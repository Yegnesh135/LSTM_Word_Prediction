# Next-Word Prediction with LSTM: Literature Edition
## Overview
Welcome to my next-word prediction project! I’ve developed an LSTM-based model to predict the next word in a sequence, trained on domain-specific literature data. This end-to-end solution showcases my expertise in deep learning, natural language processing (NLP), and deployment—delivering a creative and functional tool for text generation.

## Project Goals
**Next-Word Prediction**:     Accurately predict the next word in a sequence using a many-to-one LSTM architecture.<br>
**Domain-Specific Training**: Leverage literature texts to capture stylistic and contextual nuances.
**Interactive Deployment**:   Provide a user-friendly interface via Streamlit for real-time predictions.

## Features
**Data Preprocessing**: Tokenized and sequenced literature texts for LSTM compatibility.
**LSTM Model**:         Designed and trained a many-to-one LSTM architecture to model word dependencies.
**Domain Focus**:       Fine-tuned on literature data (e.g., classic novels, poetry) for rich, context-aware predictions.
**UI**:                 Built a Streamlit app allowing users to input text and receive next-word suggestions instantly.
**Deployment**:         Hosted the model and app on a cloud platform for seamless access.

## Tech Stack
**Languages**:       Python
**DL Framework**:    TensorFlow/Keras (LSTM implementation)
**NLP Tools**:       NLTK, Tokenizer (or specify your preprocessing library)
**Visualization**:   Matplotlib (for training metrics)
**UI**:              Streamlit
**Version Control**: Git & GitHub
